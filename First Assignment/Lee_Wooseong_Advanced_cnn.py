# -*- coding: utf-8 -*-
"""Advanced CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gCNaoBW3L_pA-tT7vTPv0K_RDGgvOG8n

신경망을 사용하는 기계학습 중 CNN은 이미지 프로세싱 등의 분야에서 좋은 성능을 발휘한다.
CNN은 기본적으로 nxn 모양의 필터 모양을 가지고 이미지의 부분을 자른 후 특징을 뽑아낸다는 특징이 있다.

Pytorch에서 제공하는 conv2d 함수는 data가 몇 겹의 데이터로 구성되어있는지를 나타내는 in_channel(cifar10과 같은 컬러 사진의 경우 RGB를 사용하기 때문에 in_channel이 3, MNIST와 같은 흑백 사진의 경우 1), 출력할 conv layer의 층을 결정하는 out_channel, 마지막으로 필터의 사이즈를 결정하는 kernal size를 입력받아 실행된다. 

![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile3.uf.tistory.com%2Fimage%2F99EC1C435B48B8481B98EC)

위의 사진을 보면 가로와 세로가 각각 32이고, 깊이가 3인 이미지가 있고, 크기가 5x5인 3겹 필터를 사용하여 activation map을 만들고 있다.
이렇게 만들어진 activation map은 28x28개의 픽셀로 이뤄져있는데 각 픽셀의 갯수는 필터의 사이즈인 5x5x3= 75이다.

이렇게 하나의 이미지에 대해 activation map을 만든 후 Pooling이라는 연산을 사용하여 activation map의 의미있는 특징들을 추출한다.

![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile29.uf.tistory.com%2Fimage%2F991526445B49831101DC96)

위의 사진은 Max Pooling이라는 기법으로 activation map을 일정한 사이즈로 나눈 후 나눠진 영역에서 가장 큰 값을 대표값으로 설정하여 정해진 사이즈의 필터에 특징을 추출해낸다.
Pooling의 방법으로는 해당 구역의 평균을 뽑아내는 average pooling과 최솟값을 뽑아내는 Min pooling등의 방법도 존재한다.

![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F999AB9425B498822317983)

이런 방식으로 Conv layer를 생성하고 그 conv layer를 pooling layer를 만든 뒤, 다시 pooling layer에 대해 conv layer를 생성하고 그 결과를 pooling하는 것을 반복하여 마지막에 fully connect layer에서 특징들을 유의미하게 분류하는 과정을 거친다. 

![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile29.uf.tistory.com%2Fimage%2F99E0713D5B498D4A1FF2D4)

이 과정을 Pytorch를 이용하여 구현하면 위의 사진과 같다. 

__init__ 함수의 fully connect layer에서는 neural network의 Linear함수를 통해 추출한 픽셀들을 0~9까지의 총 10개의 라벨에 따라 분류한다. 추출한 픽셀의 갯수는 수학적으로 계산하기보다는 아무 값이나 대입한 후 런타임 에러에서 말해주는 값으로 입력하면 된다. 

![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile30.uf.tistory.com%2Fimage%2F992ABA455B45E74E18AD88)

그리고 forward 함수에서는 마지막에 fully requested layer까지 거친 특징 값들을 위와 같은 softmax 함수를 통해 사람이 인식하기 쉬운 분류 값으로 출력받는다.

위의 과정을 train 과정을 거쳐 test해보면 MNIST에 대한 인식의 정확도가 96% 정도에 이름을 알 수 있다.

이러한 CNN의 성능을 더욱 높일 수 있도록 고안한 것이 Advanced CNN이다.
![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile27.uf.tistory.com%2Fimage%2F999A7A3F5B49B62125233E)

Advanced CNN에서는 하나의 이미지를 다양한 필터들로 conv layer를 만든 후에 그 layer들을 하나로 합친 다음 pooling layer를 만들어서 더욱 유의미한 특징 값을 추출하는 방식이다. 하나의 pooling layer를 만들 때 5x5, 3x3, 1x1 등의 다양한 커널 사이즈의 필터를 사용하기 때문에 conv layer를 만드는 과정에서 padding과 stride를 적절히 설정해줘야한다.


Advanced CNN의 핵심은 바로 conv layer를 만드는 과정에서 1x1 필터를 사용한다는 점이다. 
![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile5.uf.tistory.com%2Fimage%2F9933A3475B49C4F90A1CE7)

사진과 같이 커널 사이즈가 1인 필터를 사용하고 out_channel을 원래 이미지 in_chnnel의 절반인 32로 줄인 다음, 이 conv layer를 기반으로 다른 conv layer를 만들게 되면, 원래 이미지와 같은 width와 height의 이미지임에도 추후 conv layer를 만드는 연산의 양이 효과적으로 줄어들기 때문이다.

예를 들어, 28x28의 192체널로부터 5x5 conv layer 32체널을 만든다면 5x5x192x32의 파라미터가 만들어지겠지만,  1x1로 32체널로 줄인다면 줄이는 데에는 1x1x192x32의 파라미터가, conv layer엔 5x5x32x32의 파라미터가 필요하다.

즉, 기존에는 (5x5x28x28x192x32) 개의 연산이 필요 한 상황이라면 이제는 (1x1x28x28x192x32) + (28x28x5x5x32x32) 개의 연산이 필요하다.

![대체 텍스트](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile30.uf.tistory.com%2Fimage%2F997412365B49D6DD25877C)

이렇게 여러 개의 conv layer를 합쳐서 하나의 pooling layer로 바꾸는 과정을 Inception Module이라고 하는데, 이러한 inception module들을 이어붙이면 기존의 CNN보다 더 의미있는 특징 추출과 라벨 분류 학습이 가능하다는 게 Advanced CNN의 아이디어이다. 

Pytorch를 이용한 Advanced CNN의 구현을 아래 코드에서 설명해보겠다.
"""

import torch 
import torch.nn as nn 
import torch.nn.functional as F 
import torch.optim as optim 
from torchvision import datasets, transforms
# 데이터를 원활하게 가져오고 변형하기 위해 torchvision을 import
# transforms는 선처리를 위한 함수


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') 
batch_size = 64
# deep learning에서 분석에 사용하는 데이터는 양이 너무 방대하기 때문에 전체 데이터를 한 번의 연산으로 돌리기에는 cpu의 무리가 따름
# 때문에 전체 데이터를 몇 번에 나눠서 연산할지 결정하는 batch_size 설정
# batch_size가 클수록 연산 속도가 오래 걸림

train_dataset = datasets.MNIST(root='./mnist_data/',train = True, transform = transforms.ToTensor(),download = True) 
test_dataset = datasets.MNIST(root='./mnist_data/',train = False, transform = transforms.ToTensor()) 
# google colab의 ./data 디렉토리에 MNIST의 train과 test 데이터 다운로드
# transform 함수를 통해 다운받은 데이터를 Tensor형태로 저장함

train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True) 
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)
# DataLoader에 train_dataset과 test_dataset을 가져와서 각각 train_loader와 test_loader를 설정
# batch_size는 위에서 설정한 64의 값을 그대로 가져옴
# shuffle은 데이터 순서를 shuffle은 데이터 순서를 무작위로 섞을 것인지 결정
# shuffle값이 True면 순서를 무작위로, False면 순서대로 배치

class InceptionModule(nn.Module):
# class가 곧 생성할 Neural network
# class의 이름이 네트워크의 이름
# InceptionModule이란 하나의 이미지를 다양한 사이즈의 필터를 사용해서 특징을 추출한 conv layer들을 하나로 합쳐 max pooling하는 하나의 싸이클


                                                    # 초기화 함수
   def __init__(self,in_channels):
      super(InceptionModule,self).__init__()
      # super()에 생성할 네트워크를 저장

      self.branch1x1 = nn.Conv2d(in_channels,16,kernel_size=1) 
      # 1. in_channel의 경우 사진 데이터를 통해 입력받음
      # 2. out_channel의 수, 즉 뽑아낼 conv layer들을 16겹으로 출력
      # 3. kernel_size가 1이므로 사용할 필터의 크기가 1x1
      # 1x1 필터로 먼저 conv layer를 생성한 후 이 conv layer에 대하여 연산을 진행하면 1x1 필터를 사용하지 않을 때보다 연산을 효과적으로 줄일 수 있음

      self.branch5x5_1 = nn.Conv2d(in_channels,16,kernel_size=1) 
      self.branch5x5_2 = nn.Conv2d(16,24,kernel_size=5,padding=2)
      # 1. branch1x1과 같이 크기가 1x1인 필터를 사용하여 원래 이미지에 대한 conv layer 생성
      # 2. 1x1로 출력한 conv layer를 5x5 필터를 사용하고 24겹의 out_channel로 출력.  
      # 3. out_channels의 크기가 같아야 마지막에 모든 conv layer들을 합쳐서 사용할 수 있음. 때문에 out_channel의 사이즈를 같게 하기 위해 패딩 추가

      self.branch3x3_1 = nn.Conv2d(in_channels,16,kernel_size=1) 
      self.branch3x3_2 = nn.Conv2d(16,24,kernel_size=3,padding=1) 
      self.branch3x3_3 = nn.Conv2d(24,24,kernel_size=3,padding=1)
      # 위의 branch5x5와 같이 out_channel의 사이즈를 동일하게 conv layer 출력

      self.branch_pool = nn.Conv2d(in_channels,24,kernel_size=1) 
      # average pool 이후에 1x1 필터를 사용하는 conv_layer 출력

                                                    # forward 함수 생성
   def forward(self,x):
      branch1x1 = self.branch1x1(x)
      # __init__함수에서 초기화했던 self.branch1x1에 x를 대입하여 branch1x1에 저장

      branch5x5 = self.branch5x5_1(x) 
      branch5x5 = self.branch5x5_2(branch5x5)
      # __init__함수에서 초기화했던 self.branch5x5_1()에 x를 대입한 값을 branch5x5에 저장
      # self.branch5x5_1(x)의 출력값을 다시 self.branch5x5_2에 대입하여 brach5x5에 저장

      branch3x3 = self.branch3x3_1(x) 
      branch3x3 = self.branch3x3_2(branch3x3) 
      branch3x3 = self.branch3x3_3(branch3x3)
      # brach5x5와 같은 방법으로 conv2d 과정을 모두 거친 self.branch3x3_3(branch3x3)의 값을 branch3x3에 저장

      branch_pool = F.avg_pool2d(x,kernel_size=3,stride=1,padding=1)
      branch_pool = self.branch_pool(branch_pool) 
      # 입력 이미지에 대하여 3x3 필터를 이용하고 stride=1, padding=1을 설정하여 average_pool2d를 연산한 값을 branch_pool에 저장
      # brach_pool에 저장된 값을 __init__에서 정의한 self.branch_pool()에 대입하여 branch_pool에 저장

      outputs = [branch1x1,branch5x5,branch3x3,branch_pool]
      # 위의 연산에서 구한 1x1, 5x5, 3x3, brach_pool의 conv layer들을 outputs에 하나로 통합(concatenate)
      return torch.cat(outputs,1)

                          # MainNet 설정
                          # MainNet에서 앞서 설정한 Inception module들을 연결해주는 역할을 함
class MainNet(nn.Module):
     def __init__(self): 
       super(MainNet,self).__init__() 
       self.conv1 = nn.Conv2d(1,10,kernel_size=5)
        # MNIST는 흑백 사진이기 때문에 in_channel은 1, out_channel을 10으로, 필터의 크기는 5x5로 설정
       self.conv2 = nn.Conv2d(88,20,kernel_size=5) 
        # self.conv2는 conv1에서 연산한 결과를 입력으로 받음
        # Inception Module의 forward 함수에서 도출한 outputs의 크기가 88이기 때문에(16+24+24+24) in_channel을 88로 설정, 

       self.incept1 = InceptionModule(in_channels=10) 
      # 위에서 설정한 InceptionModule 클래스에 in_channels를 10으로 설정하여 실행
       self.incept2 = InceptionModule(in_channels=20) 
      # InceptionModule 클래스에 in_channels를 20으로 설정하여 실행

       self.max_pool = nn.MaxPool2d(2)
      # max_pooling의 기본 설정값인 2x2로 연산한 activation map을 max_pooling 
       self.fc = nn.Linear(1408,10)
      # 전체 프로그램을 실행하면 나오는 에러를 통해서 최종 연산의 차원인 1408을 확인한 후 대입하여 fully connect의 input으로 넘겨줌

     def forward(self,x):
      in_size = x.size(0) 
      # size(0)을 하면 (n, 28*28)중 n을 리턴함 
      x = F.relu(self.max_pool(self.conv1(x)))
      # 위에서 정의한 conv1에 x를 대입하고 max_pooling을 통해 특징을 뽑아낸 데이터를 ReLu를 통해 activation
      # Sigmoid 함수의 경우 결과가 0~1 사이에서 값이 머무름.
      # 때문에 neural net의 hidden layer 수가 많을 때, 역전파 과정에서 weight값이 원래의 입력에 미치는 영향이 거의 미미해지는 vanishing gradients현상이 발생
      # 때문에 학습 layer의 수가 많을 때는 Sigmoid보다 ReLu 함수가 비교적 효율적임

      x = self.incept1(x)
      # conv1을 ReLu를 통해 activation한 결과를 위에서 정의한 self.incept1, 즉 Inception Module에 대입 
      x = F.relu(self.max_pool(self.conv2(x)))
      # 위에서 정의한 self.conv2에 앞서 연산한 x를 대입한 후 __init__에서 정의한 대로 2x2 크기로 Max Pooling 실행하여 x에 저장
      x = self.incept2(x) 
      # 마지막으로 self.incept2()에 위에서 과정을 거친 x 대입
      x = x.view(in_size,-1)
      # in_channel의 값만 그대로 두고 나머지 값들은 -1로 펴버림
      # batch_size * channel * width * height였다면
      # batch_size, (channel * width * height)의 형태로
      x = self.fc(x)
      # conv2d 과정과 max_pooling을 마친 1408개의 cell들에 대하여 fully connected 실행
      return F.log_softmax(x)
      # 연산된 값들을 softmax activation을 통하여 출력

model = MainNet().to(device)
# 연산에 GPU를 사용하기 위한 과정
optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5)
# 스토케스틱 경사 하강법을 사용할 때 파라미터 값에 꼭 사용하는 model의 parameters()를 입력해줘야 함
# 학습률을 0.01로 설정하여 train 과정의 역전파에서 사용.
# 수정할 때의 momentum은 0.5로 설정
criterion = nn.NLLLoss()
# forward의 return값으로 연산을 마친 fully connected layer에 softmax 함수를 적용했으므로 NLLLoss()를 통해 라벨 분류 마무리.

# train 과정
def train(epoch): 
  model.train() 
  for batch_idx, (data, target) in enumerate(train_loader):
  # 프로그램 처음에 설정했던 train_loader에서 데이터를 가져옴
     data = data.to(device) 
     target = target.to(device) 
     
     output = model(data) 
     optimizer.zero_grad() 
     #네트워크 시작할 때 네트워크의 각 파라미터의 gradient를 0으로 초기화해줌
     loss = criterion(output, target) 
     loss.backward()
     # 계산된 loss값, 즉 원래의 라벨과 예상값의 차이에서 발생하는 loss들에 대한 역전파 
     optimizer.step() 
     # 역전파를 통해 얻은 gradient를 optimzer.step()을 통해 업데이트
     if batch_idx % 50 == 0:
        print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.data.item())) 

# test 과정
def test(): 
  model.eval() 
  test_loss = 0 
  correct = 0 
  for data, target in test_loader:
     data = data.to(device) 
     target = target.to(device) 
     output = model(data)
     test_loss += criterion(output, target).data.item()
     # get the index of the max log-probability 
     pred = output.data.max(1, keepdim=True)[1] 
     correct += pred.eq(target.data.view_as(pred)).cpu().sum() 
     
     test_loss /= len(test_loader.dataset)/batch_size 
     print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))

for epoch in range(1, 10):
# epoch는 총 train과 test를 몇 번 실행할지 결정
   train(epoch) 
   test()
# 9번의 train 이후의 test 값을 보게 되면 정확도가 98%에 이르는 걸 확인할 수 있다.