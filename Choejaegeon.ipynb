{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Choejaegeon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choe1842/ai-security/blob/master/Choejaegeon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdQ37XxyaXm5",
        "colab_type": "code",
        "outputId": "c15ec67c-9db7-405c-9bfe-0129ae294527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Gradient Descent 구현 하기\n",
        "\n",
        "import torch \n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [3.0, 6.0, 9.0]\n",
        "\n",
        "w = 1 # 임의의 값 지정\n",
        "\n",
        "def forward(x): \n",
        "  return x * w # 기본 식\n",
        "\n",
        "def loss(x,y): # loss 계산\n",
        "  y_pred = forward(x)\n",
        "  return (y_pred - y) * (y_pred - y) # 예측 값과 실제 값의 거리 차를 돌려줌\n",
        "\n",
        "def gradient(x,y): \n",
        "  return 2 * x * (x * w - y) # gradient를 계산함\n",
        "\n",
        "print(\"학습 전\", \"값 :\", forward(4))\n",
        "\n",
        "for epoch in range(15): # 학습 과정\n",
        "  for x_val, y_val in zip(x_data, y_data):\n",
        "    grad = gradient(x_val, y_val)\n",
        "    \n",
        "    w = w - 0.01 * grad # 가중치 업데이트\n",
        "    \n",
        "    print(\"\\tgrad:\", x_val, y_val, round(grad,2))\n",
        "    l = loss(x_val, y_val)\n",
        "   \n",
        "  print(\"prograss:\", epoch, \"w = \", round(w,2), \"loss = \", round(l,5))\n",
        "  \n",
        " \n",
        "print(\"학습완료\", \"값 :\", forward(4)) # 학습 확인\n",
        "\n",
        "\n",
        "# Advanced Convolutional neural network 구현하기\n",
        "\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn # 딥러닝 모델에 필요한 모듈이 모아져있는 패키지\n",
        "import torch.nn.functional as F # 함수의 input으로 반드시 연산되어야 하는 값을 받는 기능\n",
        "import torch.optim as optim # 최적화 방법이 있는 패키지\n",
        "from torchvision import datasets, transforms # torch에서 제공해주는 데이터셋 가져오기\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # gpu 사용 가능 확인\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',train = True, transform = transforms.ToTensor(),download = True) # 학습용 데이터셋\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',train = False, transform = transforms.ToTensor()) # 학습 정도를 테스트하기 위한 데이터셋\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)  # 학습 데이터를 받아오는 함수\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False) # 테스트 데이터를 받아오는 함수\n",
        "\n",
        "class InceptionModule(nn.Module): # 여러 필터의 결과를 합치기 위한 인셉션 모듈 구현, nn.Module 상속\n",
        "\n",
        "  def __init__(self,in_channels): # 네트워크 계층 정의\n",
        "    super(InceptionModule,self).__init__() \n",
        "    self.branch1x1 = nn.Conv2d(in_channels,16,kernel_size=1) # _1 : 연산을 줄이면서 한 레이어에서 더 깊은 논리 처리 가능\n",
        "  \n",
        "    self.branch5x5_1 = nn.Conv2d(in_channels,16,kernel_size=1) # _1 과 같은 기능 \n",
        "    self.branch5x5_2 = nn.Conv2d(16,24,kernel_size=5,padding=2) # 16채널을 5X5의 24개 채널로 만듦, 패딩을 통해 결과값이 작아지는 것을 방지\n",
        "  \n",
        "    self.branch3x3_1 = nn.Conv2d(in_channels,16,kernel_size=1) # _1 과 같은 기능\n",
        "    self.branch3x3_2 = nn.Conv2d(16,24,kernel_size=3,padding=1) # 16채널을 3X3의 24개 채널로 만듦, 패딩을 통해 결과값이 작아지는 것을 방지\n",
        "    self.branch3x3_3 = nn.Conv2d(24,24,kernel_size=3,padding=1) # 24채널을 3X3의 24개 채널로 만듦, 패딩을 통해 결과값이 작아지는 것을 방지\n",
        "  \n",
        "    self.branch_pool = nn.Conv2d(in_channels,24,kernel_size=1) \n",
        "\n",
        "  def forward(self,x): # 하나의 x를 4개로 쪼개주기 위함\n",
        "    \n",
        "    branch1x1 = self.branch1x1(x) \n",
        "    \n",
        "    branch5x5 = self.branch5x5_1(x) \n",
        "    branch5x5 = self.branch5x5_2(branch5x5) \n",
        "    \n",
        "    branch3x3 = self.branch3x3_1(x) \n",
        "    branch3x3 = self.branch3x3_2(branch3x3) \n",
        "    branch3x3 = self.branch3x3_3(branch3x3) \n",
        "    \n",
        "    branch_pool = F.avg_pool2d(x,kernel_size=3,stride=1,padding=1) \n",
        "    branch_pool = self.branch_pool(branch_pool) \n",
        "    \n",
        "    outputs = [branch1x1,branch5x5,branch3x3,branch_pool]\n",
        "    return torch.cat(outputs, 1) # 4개의 함수를 하나로 합쳐준다. 채널의 개수는 총 88개(16+24+24+24)\n",
        "  \n",
        "  \n",
        "  \n",
        "class MainNet(nn.Module): # 전체 모델 구성\n",
        "  def __init__(self): \n",
        "    super(MainNet,self).__init__() \n",
        "    self.conv1 = nn.Conv2d(1,10,kernel_size=5)  # 5x5의 Conv layer 10채널 통과\n",
        "    \n",
        "    self.conv2 = nn.Conv2d(88,20,kernel_size=5) # # 5x5의 Conv layer 20채널 통과, 인셉션 모델의 채널의 개수가 총 88개 이기 때문에 채널을 88개로 함\n",
        "    \n",
        "    self.incept1 = InceptionModule(in_channels=10) # 인셉션 모듈 통과\n",
        "    self.incept2 = InceptionModule(in_channels=20) # 인셉션 모듈 통과\n",
        "    \n",
        "    self.max_pool = nn.MaxPool2d(2)  # 주어진 필터 범위 내에서 최대값을 뽑아내는 과정\n",
        "    self.fc = nn.Linear(1408,10) # 마지막 Conv layer의 cell을 한줄로 나열한 후 그 개수에 맞추어 Fully connected layer의 input으로 넘겨주는 것, 즉, 1408개의 데이터를 10개로 바꿈\n",
        "\n",
        "  def forward(self,x): \n",
        "    in_size = x.size(0) # size(0)을 하면 (n, 28*28)중 n을 리턴함 \n",
        "    x = F.relu(self.max_pool(self.conv1(x))) # 학습을 위한 활성화 함수 relu\n",
        "    x = self.incept1(x) \n",
        "    x = F.relu(self.max_pool(self.conv2(x))) \n",
        "    x = self.incept2(x) \n",
        "    x = x.view(in_size,-1) \n",
        "    x = self.fc(x) \n",
        "    return F.log_softmax(x) # 아웃풋을 한번에 묶어 결과를 조정해주는 소프트맥스 함수 사용\n",
        "\n",
        "model = MainNet().to(device) # 모델 정의\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5) # 가중치를 업데이트 시켜주는 최적화 (Stochastic Gradient Descent) 사용, 과도한 연산 방지\n",
        "criterion = nn.NLLLoss() # loss 함수 정의, softmax함수를 사전에 사용했기 때문에, NLLLoss를 사용합니다.\n",
        "\n",
        "def train(epoch): \n",
        "  model.train() \n",
        "  for batch_idx, (data, target) in enumerate(train_loader): \n",
        "    data = data.to(device) \n",
        "    target = target.to(device) \n",
        "    \n",
        "    output = model(data) # 데이터를 학습 모델에 투입\n",
        "    \n",
        "    optimizer.zero_grad() # gradient buffers를 0으로\n",
        "    loss = criterion(output, target) # loss 계산\n",
        "    loss.backward()  # 역전파 과정을 통해 각 변수마다 loss에 대한 gradient를 구해줌\n",
        "    optimizer.step() # model의 파라미터들을 업데이트 해주는 함수\n",
        "    if batch_idx % 50 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.data))\n",
        "\n",
        "#batch: 전체 데이터를 나눠 일부를 묶은 것을 의미함\n",
        "#epoch: 모든 학습 데이터에 대해 forward 와 backward pass를 한번 진행한 상태\n",
        "\n",
        "def test(): # 과학습 가능성을 고려하여 별도의 테스트 함수 선언\n",
        "  model.eval() \n",
        "  test_loss = 0 \n",
        "  correct = 0 \n",
        "  for data, target in test_loader: # 테스트 데이터 사용\n",
        "    data = data.to(device) \n",
        "    target = target.to(device) \n",
        "    output = model(data) \n",
        "    test_loss += criterion(output, target).data # loss 계산\n",
        "    pred = output.data.max(1, keepdim=True)[1] \n",
        "    correct += pred.eq(target.data.view_as(pred)).cpu().sum() #pred배열과 data의 일치 여부 검사해주는 기능\n",
        "   \n",
        "  test_loss /= len(test_loader.dataset)/batch_size \n",
        "  print('\\nTest set: 평균 loss: {:.4f}, 정확도: {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "for epoch in range(1, 10): # 최종 실행\n",
        "  train(epoch)\n",
        "  test()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 전 값 : 4\n",
            "\tgrad: 1.0 3.0 -4.0\n",
            "\tgrad: 2.0 6.0 -15.68\n",
            "\tgrad: 3.0 9.0 -32.46\n",
            "prograss: 0 w =  1.52 loss =  19.67696\n",
            "\tgrad: 1.0 3.0 -2.96\n",
            "\tgrad: 2.0 6.0 -11.59\n",
            "\tgrad: 3.0 9.0 -24.0\n",
            "prograss: 1 w =  1.91 loss =  10.75508\n",
            "\tgrad: 1.0 3.0 -2.19\n",
            "\tgrad: 2.0 6.0 -8.57\n",
            "\tgrad: 3.0 9.0 -17.74\n",
            "prograss: 2 w =  2.19 loss =  5.87853\n",
            "\tgrad: 1.0 3.0 -1.62\n",
            "\tgrad: 2.0 6.0 -6.34\n",
            "\tgrad: 3.0 9.0 -13.12\n",
            "prograss: 3 w =  2.4 loss =  3.2131\n",
            "\tgrad: 1.0 3.0 -1.2\n",
            "\tgrad: 2.0 6.0 -4.68\n",
            "\tgrad: 3.0 9.0 -9.7\n",
            "prograss: 4 w =  2.56 loss =  1.75622\n",
            "\tgrad: 1.0 3.0 -0.88\n",
            "\tgrad: 2.0 6.0 -3.46\n",
            "\tgrad: 3.0 9.0 -7.17\n",
            "prograss: 5 w =  2.67 loss =  0.95992\n",
            "\tgrad: 1.0 3.0 -0.65\n",
            "\tgrad: 2.0 6.0 -2.56\n",
            "\tgrad: 3.0 9.0 -5.3\n",
            "prograss: 6 w =  2.76 loss =  0.52468\n",
            "\tgrad: 1.0 3.0 -0.48\n",
            "\tgrad: 2.0 6.0 -1.89\n",
            "\tgrad: 3.0 9.0 -3.92\n",
            "prograss: 7 w =  2.82 loss =  0.28678\n",
            "\tgrad: 1.0 3.0 -0.36\n",
            "\tgrad: 2.0 6.0 -1.4\n",
            "\tgrad: 3.0 9.0 -2.9\n",
            "prograss: 8 w =  2.87 loss =  0.15675\n",
            "\tgrad: 1.0 3.0 -0.26\n",
            "\tgrad: 2.0 6.0 -1.03\n",
            "\tgrad: 3.0 9.0 -2.14\n",
            "prograss: 9 w =  2.9 loss =  0.08568\n",
            "\tgrad: 1.0 3.0 -0.2\n",
            "\tgrad: 2.0 6.0 -0.76\n",
            "\tgrad: 3.0 9.0 -1.58\n",
            "prograss: 10 w =  2.93 loss =  0.04683\n",
            "\tgrad: 1.0 3.0 -0.14\n",
            "\tgrad: 2.0 6.0 -0.57\n",
            "\tgrad: 3.0 9.0 -1.17\n",
            "prograss: 11 w =  2.95 loss =  0.0256\n",
            "\tgrad: 1.0 3.0 -0.11\n",
            "\tgrad: 2.0 6.0 -0.42\n",
            "\tgrad: 3.0 9.0 -0.87\n",
            "prograss: 12 w =  2.96 loss =  0.01399\n",
            "\tgrad: 1.0 3.0 -0.08\n",
            "\tgrad: 2.0 6.0 -0.31\n",
            "\tgrad: 3.0 9.0 -0.64\n",
            "prograss: 13 w =  2.97 loss =  0.00765\n",
            "\tgrad: 1.0 3.0 -0.06\n",
            "\tgrad: 2.0 6.0 -0.23\n",
            "\tgrad: 3.0 9.0 -0.47\n",
            "prograss: 14 w =  2.98 loss =  0.00418\n",
            "학습완료 값 : 11.913800188063524\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:112: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.300515\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.283001\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.157422\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.918620\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.596030\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.520006\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.427651\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.386381\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.305072\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.288072\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.104975\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.252903\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.232002\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.130033\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.366724\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.197367\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.347452\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.109280\n",
            "\n",
            "Test set: 평균 loss: 0.1597, 정확도: 9513/10000 (95%) \n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.227709\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.082403\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.123265\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.166074\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.114319\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.116525\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.080550\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.126044\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.300343\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.069029\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.173760\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.076700\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.141211\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.066474\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.120049\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.111390\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.300051\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.189401\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.087515\n",
            "\n",
            "Test set: 평균 loss: 0.0912, 정확도: 9713/10000 (97%) \n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.149139\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.055755\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.127663\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.171443\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.037417\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.037799\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.071309\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.136987\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.045097\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.124844\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.078612\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.280085\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.105127\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.091301\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.082380\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.213366\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.114288\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.085617\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.142529\n",
            "\n",
            "Test set: 평균 loss: 0.0693, 정확도: 9773/10000 (97%) \n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.153151\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.079060\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.109117\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.031161\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.029336\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.056902\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.098950\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.056514\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.027098\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.036803\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.088806\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.089686\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.034245\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.110525\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.017243\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.110223\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.057518\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.035130\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.038882\n",
            "\n",
            "Test set: 평균 loss: 0.0595, 정확도: 9820/10000 (98%) \n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.034653\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.048813\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.056513\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.020517\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.133824\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.038997\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.089257\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.059562\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.026787\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.123496\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.015389\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.161023\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.029176\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.055948\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.076917\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.012511\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.016164\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.036828\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.041651\n",
            "\n",
            "Test set: 평균 loss: 0.0534, 정확도: 9838/10000 (98%) \n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.092038\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.106452\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.029223\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.068436\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.061390\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.012756\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.004330\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.135567\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.040720\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.265864\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.033971\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.056867\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.038863\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.022161\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.029982\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.025015\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.046249\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.060640\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.060831\n",
            "\n",
            "Test set: 평균 loss: 0.0557, 정확도: 9824/10000 (98%) \n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.076669\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.205018\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.047663\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.172287\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.012489\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.012579\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.017649\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.026899\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.092659\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.010049\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.034012\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.050915\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.030493\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.046260\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.119630\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.033446\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.011659\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.106592\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.119183\n",
            "\n",
            "Test set: 평균 loss: 0.0476, 정확도: 9846/10000 (98%) \n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.025336\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.017961\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.029381\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.009125\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.068871\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.047237\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.009472\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.026912\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.040753\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.137633\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.005862\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.075867\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.025675\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.038185\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.044855\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.034133\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.034580\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.073363\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.107760\n",
            "\n",
            "Test set: 평균 loss: 0.0496, 정확도: 9836/10000 (98%) \n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.127745\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.033875\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.082772\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.101310\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.016145\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.008492\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.014312\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.026548\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.060378\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.020907\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.012151\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.068214\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.048314\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.042501\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.125772\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.027475\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.010406\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.019192\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.030351\n",
            "\n",
            "Test set: 평균 loss: 0.0451, 정확도: 9858/10000 (98%) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}